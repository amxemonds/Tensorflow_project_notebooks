{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Skills Checklist: Preparing for the Google TensorFlow Certification Exam"
      ],
      "metadata": {
        "id": "AiD7HQ1M2Bed"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TensorFlow developer skills"
      ],
      "metadata": {
        "id": "gn_mLTqf2EpW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fDPyGLze19FZ"
      },
      "outputs": [],
      "source": [
        "# know how to program in python, resolve python issues, compile and run\n",
        "# python programs in PyCharm\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# know how to find information about tensorflow APIs, including how\n",
        "# to find guides and API references on tensorflow.org\n"
      ],
      "metadata": {
        "id": "iwSFVroH2Kxr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# know how to debug, investigate, and solve error messages from the\n",
        "# tensorflow API\n"
      ],
      "metadata": {
        "id": "YKMnZmKc2Zb1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# know how to search beyond tensorflow.org, as and when necessary, to\n",
        "# solve your tensorflow questions\n"
      ],
      "metadata": {
        "id": "5bIOIRE72eKc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# know how to create ML models using tensorflow where the model size\n",
        "# is reasonable for the problem being solved\n"
      ],
      "metadata": {
        "id": "KMb6DU762n4w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# know how to save ML models and check the model file size\n",
        "\n",
        "from pathlib import Path\n",
        "Path('somefile.txt').stat().st_size\n"
      ],
      "metadata": {
        "id": "OgLWUrrb2tER"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# understand the compatibility discrepancies between different versions\n",
        "# of tensorflow\n"
      ],
      "metadata": {
        "id": "cnloUusv2vzv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ensure that inputs to a model are in the correct shape\n"
      ],
      "metadata": {
        "id": "MY4Ez5HJ3hDn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ensure that you can match test data to the input shape of a \n",
        "# neural network\n"
      ],
      "metadata": {
        "id": "xIZ4KtLk3kgG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ensure that you can match output data of a neural network to\n",
        "# specified input shape for test data\n"
      ],
      "metadata": {
        "id": "4z5EQTFH3pGR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# understand batch loading of data\n"
      ],
      "metadata": {
        "id": "ZYS-7T4_3u-9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# use callbacks to trigger the end of training cycles\n",
        "\"\"\"\n",
        "see \"Callback construction\" below"
      ],
      "metadata": {
        "id": "oB9QLWzJ3wot"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# use datasets from different sources\n"
      ],
      "metadata": {
        "id": "AnkSJnc43y9u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# use datasets in different formats, including json and csv\n",
        "\n",
        "# CSV FORMAT\n",
        "insurance = pd.read_csv(\"https://raw.githubusercontent.com/stedy/Machine-Learning-with-R-datasets/master/insurance.csv\")\n",
        "# turns it right into a pandas DataFrame!\n",
        "\n",
        "# JSON FORMAT\n",
        "import json\n",
        "\n",
        "f = open(\"data.json\",)\n",
        "data = json.load(f) # returns JSON object as a dictionary\n"
      ],
      "metadata": {
        "id": "SVb7x_7232AS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building and training neural network models using TensorFlow 2.x"
      ],
      "metadata": {
        "id": "xNd6E_862471"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# use tensorflow 2.x\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "metadata": {
        "id": "WV-wlbz724YX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# build, compile, and train ML models using tensorflow\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    layers.Dense(4,activation=\"relu\"),\n",
        "    layers.Dense(4,activation=\"relu\"),\n",
        "    layers.Dense(1,activation=\"sigmoid\")\n",
        "],name=\"model\")\n",
        "\n",
        "model.compile(loss=\"binary_crossentropy\",\n",
        "              optimizer=tf.keras.optimizers.Adam(),\n",
        "              metrics=[\"accuracy\"])\n",
        "\n",
        "model.fit(X_train,y_train,epochs=5)\n"
      ],
      "metadata": {
        "id": "SajQjFFz3BnH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### predicting results"
      ],
      "metadata": {
        "id": "ssDGGy1fBK6w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# use models to predict results\n",
        "y_preds = model.predict(X_test)\n"
      ],
      "metadata": {
        "id": "iQbx7l2a3IVA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# build sequential models with multiple layers\n",
        "\"\"\"\n",
        "see above"
      ],
      "metadata": {
        "id": "pBjYfcaI3JvP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# build and train models for binary classification\n",
        "\"\"\"\n",
        "output layer must be layers.Dense(1,activation=\"sigmoid\")\n",
        "loss = \"binary_crossentropy\""
      ],
      "metadata": {
        "id": "5XvefoL23NBH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# build and train models for multi-class categorization\n",
        "\"\"\"\n",
        "output layer must be layers.Dense(NUM_CLASSES,activation=\"softmax\")\n",
        "loss = \"categorical_crossentropy\""
      ],
      "metadata": {
        "id": "JWq4lSFG3PSd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plot loss and accuracy of a trained model\n",
        "history = model.fit(...)\n",
        "pd.DataFrame(history.history).plot(figsize=(10,7),xlabel=\"epochs\")\n"
      ],
      "metadata": {
        "id": "K3lA7sLe3SGU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Image classification\n",
        "\n",
        "steps:\n",
        "1. **read in data**\n",
        "2. **find ```train_dir``` and ```test_dir```**\n",
        "3. **create ```train_datagen``` and ```test_datagen``` using ```ImageDataGenerator```, passing augmentation arguments for rescaling, etc.**\n",
        "4. **create ```train_data``` and ```test_data``` using e.g. ```train_datagen.flow_from_directory()```**\n",
        "    * ```shuffle=True```\n",
        "    * ```batch_size=32```\n",
        "    * ```class_model=\"binary\"``` or ```\"categorical\"```\n",
        "    * choose a target image size\n",
        "5. **design model**\n",
        "    * some combination of stacked ```layers.Conv2D()``` and ```layers.MaxPool2D()```\n",
        "    * ```layers.Flatten()```\n",
        "    * if binary, ```layers.Dense(1,activation=\"sigmoid\")```\n",
        "    * if multiclass, ```layers.Dense(num_classes,activation=\"softmax\")```\n",
        "6. **compile**\n",
        "    * if binary, ```loss=\"binary_crossentropy\"```\n",
        "    * if multiclass, ```loss=\"categorical_crossentropy\"```\n",
        "    * ```metrics=[\"accuracy\"]```\n",
        "7. **fit model**\n",
        "8. **reload stored best weights for model**\n",
        "9. **evaluate**\n",
        "10. **predict**\n"
      ],
      "metadata": {
        "id": "b4eUVDDQ4MEs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### getting image data (TFDS)"
      ],
      "metadata": {
        "id": "EEgHLH-OJEpw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow_datasets as tfds\n",
        "\n",
        "datasets_list = tfds.list_builders()\n",
        "print(\"food101\" in datasets_list)\n",
        "\n",
        "# Load in the data (takes about 5-6 minutes in Google Colab)\n",
        "(train_data, test_data), ds_info = tfds.load(name=\"food101\", # target dataset to get from TFDS\n",
        "                                             split=[\"train\", \"validation\"], # what splits of data should we get? note: not all datasets have train, valid, test\n",
        "                                             shuffle_files=True, # shuffle files on download?\n",
        "                                             as_supervised=True, # download data in tuple format (sample, label), e.g. (image, label)\n",
        "                                             with_info=True) # include dataset metadata? if so, tfds.load() returns tuple (data, ds_info)\n",
        "\n",
        "ds_info.features\n",
        "class_names = ds_info.features[\"label\"].names\n",
        "\n",
        "# Take one sample off the training data\n",
        "train_one_sample = train_data.take(1)\n",
        "\n",
        "# Output info about our training sample\n",
        "for image, label in train_one_sample:\n",
        "  print(f\"Image shape: {image.shape}\\nImage dtype: {image.dtype}\")\n",
        "\n",
        "\n",
        "# Make a function for preprocessing images\n",
        "def preprocess_img(image, label):\n",
        "    image = tf.image.resize(image, [224,224]) # reshape to img_shape\n",
        "    return tf.cast(image, tf.float32), label # return (float32_image, label) tuple\n"
      ],
      "metadata": {
        "id": "AUCLZnbwJHtu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# batching datasets with TFDS\n",
        "train_data = train_data.map(map_func=preprocess_img, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "train_data = train_data.shuffle(buffer_size=1000).batch(batch_size=32).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "\n",
        "test_data = test_data.map(preprocess_img, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "test_data = test_data.batch(32).prefetch(tf.data.AUTOTUNE)\n"
      ],
      "metadata": {
        "id": "TiEVFQXULvln"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model with TFDS\n",
        "base_model = tf.keras.applications.EfficientNetB0(include_top=False)\n",
        "base_model.trainable = False\n",
        "\n",
        "inputs = layers.Input(shape=(224,224,3))\n",
        "# x = layers.Rescaling(1./255)(inputs) # only for ResNet\n",
        "x = base_model(inputs) # training=False if rescaling layer\n",
        "x = layers.GlobalAveragePooling2D()(x)\n",
        "x = layers.Dense(len(class_names))(x)\n",
        "# separate activatiion of output layer so we can output float32 activations\n",
        "outputs = layers.Activation(\"softmax\",dtype=tf.float32)(x)\n",
        "model = tf.keras.Model(inputs,outputs,name=\"model\")\n",
        "\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
        "              optimizer=tf.keras.optimizers.Adam(),\n",
        "              metrics=[\"accuracy\"])\n",
        "\n",
        "model.fit(train_data,\n",
        "          epochs=3,\n",
        "          steps_per_epoch=len(train_data),\n",
        "          validation_data=test_data,\n",
        "          validation_steps=len(test_data),\n",
        "          callbacks=[])\n"
      ],
      "metadata": {
        "id": "EmStoaNeNzbs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### getting image data (tf.keras.datasets.fashion_mnist)"
      ],
      "metadata": {
        "id": "otVaSb53uUaP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# use datasets from tf.data.datasets\n",
        "from tensorflow.keras.datasets import fashion_mnist\n",
        "(train_data,train_labels),(test_data,test_labels) = fashion_mnist.load_data()\n",
        "\n",
        "print(train_data.shape)\n",
        "print(test_data.shape)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.imshow(train_data[0],cmap=plt.cm.binary)\n",
        "print(train_labels[0])\n",
        "\n",
        "# you can look on the dataset's github page for label translation"
      ],
      "metadata": {
        "id": "s-kO-u7A4Jcm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### building and training a simple model (tf.keras.datasets.fashion_mnist)"
      ],
      "metadata": {
        "id": "hfkEygHex1PD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# build and train models to process real-world image datasets\n",
        "# with fashion_mnist dataset:\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    layers.Flatten(input_shape=(n,n)), # one image is n pixels x n pixels\n",
        "    layers.Dense(4,activation=\"relu\"),\n",
        "    layers.Dense(4,activation=\"relu\"),\n",
        "    layers.Dense(NUM_CATEGORIES,activation=\"softmax\")\n",
        "],name=\"model\")\n",
        "\n",
        "# the provided labels are in integer format, not one-hot, so we use \"sparse\"\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
        "              optimizer=tf.keras.optimizers.Adam(),\n",
        "              metrics=[\"accuracy\"])\n",
        "\n",
        "model.fit(x=train_data,\n",
        "          y=train_labels,\n",
        "          epochs=10,\n",
        "          validation_data=(test_data,test_labels))\n",
        "\n",
        "# normalized data works better\n",
        "train_data = train_data/255. # if RGB values are not already 0 to 1\n",
        "test_data = test_data/255.\n",
        "\n",
        "# use LearningRateScheduler to plot loss across learning rates\n",
        "# use plt.semilogx(lrs,history.history[\"loss\"])\n"
      ],
      "metadata": {
        "id": "3U0AI0v4BEZb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### getting image data (pizza_steak unzip)"
      ],
      "metadata": {
        "id": "DHBR-JwEuMJm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "\n",
        "# Download zip file of pizza_steak images\n",
        "!wget https://storage.googleapis.com/ztm_tf_course/food_vision/pizza_steak.zip \n",
        "\n",
        "# Unzip the downloaded file\n",
        "zip_ref = zipfile.ZipFile(\"pizza_steak.zip\", \"r\")\n",
        "zip_ref.extractall()\n",
        "zip_ref.close()\n",
        "\n",
        "num_steak_images_train = len(os.listdir(\"pizza_steak/train/steak\"))\n"
      ],
      "metadata": {
        "id": "o_FoFPxMugBL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get the class names\n",
        "import pathlib\n",
        "import numpy as np\n",
        "\n",
        "data_dir = pathlib.Path(\"pizza_steak/train/\") # turn our training path into a Python path\n",
        "class_names = np.array(sorted([item.name for item in data_dir.glob('*')])) # created a list of class_names from the subdirectories\n",
        "print(class_names)\n"
      ],
      "metadata": {
        "id": "jVz4X2_fuuRe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# view image\n",
        "import matplotlib.image as mpimg\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "\n",
        "random_image = random.sample(os.listdir(\"pizza_steak/train/steak/\"),1)\n",
        "img = mpimg.imread(\"pizza_steak/train/steak/\"+random_image[0])\n",
        "plt.imshow(img)\n"
      ],
      "metadata": {
        "id": "cLdb2GoYuzGI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### using a real-world image file"
      ],
      "metadata": {
        "id": "DbJXNIPm58tA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# use real-world images in different shapes and sizes (adjust in ImageDataGenerator as target_size=(n,n))\n",
        "print(img.shape)\n",
        "\n",
        "# input_shape definition for Sequential models\n",
        "# input_shape = (batch_size,image_height,image_width,color_channels)\n",
        "\n",
        "# import a single image and prepare it for prediction\n",
        "!wget https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/images/03-steak.jpeg \n",
        "steak = mpimg.imread(\"03-steak.jpeg\")\n",
        "plt.imshow(steak)\n",
        "plt.axis(False);\n",
        "\n",
        "img = tf.io.read_file(\"03-steak.jpeg\")\n",
        "img = tf.image.decode_image(img,channels=3) # restrict to 3 color channels (because that's what our model trains on)\n",
        "img = tf.image.resize(img,size=[224,224])\n",
        "img = img/255.\n",
        "\n",
        "# to compensate for the missing batch size in the input shape, use tf.expand_dims()\n",
        "model.predict(tf.expand_dims(img,axis=0))\n"
      ],
      "metadata": {
        "id": "b6Mt7MRNBMlU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### using ImageDataGenerator (pizza_steak example)"
      ],
      "metadata": {
        "id": "6Dgj8T-2yJDP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# build and train models to process real-world image datasets\n",
        "# with ImageDataGenerator:\n",
        "\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# binary problem:\n",
        "train_dir = \"pizza_steak/train/\"\n",
        "train_datagen = ImageDataGenerator(rescale=1./255)\n",
        "train_data = train_datagen.flow_from_directory(train_dir,\n",
        "                                               batch_size=32,\n",
        "                                               target_size=(224,224), # convert all images to my desired size\n",
        "                                               class_mode=\"binary\", # type of problem\n",
        "                                               shuffle=True)\n",
        "\n",
        "test_dir = \"pizza_steak/test/\"\n",
        "val_datagen = ImageDataGenerator(rescale=1./255)\n",
        "val_data = val_datagen.flow_from_directory(test_dir,\n",
        "                                               batch_size=32,\n",
        "                                               target_size=(224,224), # convert all images to my desired size\n",
        "                                               class_mode=\"binary\", # type of problem\n",
        "                                               shuffle=True)\n",
        "\n",
        "# can get a sample of the training data batch as:\n",
        "images,labels = train_data.next()\n",
        "len(images),len(labels)\n"
      ],
      "metadata": {
        "id": "nDem_XEpBR0e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for a multiclass problem, change class_mode to \"categorical\"\n",
        "train_datagen.flow_from_directory(train_dir,\n",
        "                                  batch_size=32,\n",
        "                                  target_size=(224,224), # convert all images to my desired size\n",
        "                                  class_mode=\"categorical\", # type of problem\n",
        "                                  shuffle=True)"
      ],
      "metadata": {
        "id": "oDVef1x169DK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# understand how ImageDataGenerator labels images based on the \n",
        "# directory structure\n",
        "\"\"\"\n",
        "see above, \"use ImageDataGenerator\""
      ],
      "metadata": {
        "id": "wHNYX3ciBUIp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### an alternative to ImageDataGenerator (tf.keras.preprocessing.image_dataset_from_directory)"
      ],
      "metadata": {
        "id": "lukejOwrCSMo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# you can also use image_data_from_directory if you want to...\n",
        "# it's faster, but they may not approve.\n",
        "\n",
        "# multiclass example:\n",
        "train_data = tf.keras.preprocessing.image_dataset_from_directory(train_dir,\n",
        "                                                                 image_size=(224,224),\n",
        "                                                                 label_model=\"categorical\", # type of labels you have\n",
        "                                                                 batch_size=32)\n",
        "\n",
        "test_data = tf.keras.preprocessing.image_dataset_from_directory(test_dir,\n",
        "                                                                 image_size=(224,224),\n",
        "                                                                 label_model=\"categorical\", # type of labels you have\n",
        "                                                                 shuffle=False,\n",
        "                                                                 batch_size=32) # probably don't need to batch test data\n",
        "\n",
        "train_data.class_names\n",
        "images,labels = train_data.take(1)\n",
        "# once again, labels are one-hot encoded.\n"
      ],
      "metadata": {
        "id": "8CZSx3mA9pt-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### augmenting data"
      ],
      "metadata": {
        "id": "T76HO5VaCf3c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# identify strategies to prevent overfitting, including augmentation\n",
        "# and dropout\n"
      ],
      "metadata": {
        "id": "p-fZhw9w3Xmw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# use image augmentation to prevent overfitting\n",
        "# note: you don't augment test data\n",
        "\n",
        "# to prevent overfitting, add complexity to ImageDataGenerator\n",
        "train_datagen_augmented = ImageDataGenerator(rescale=1/255.,\n",
        "                                             rotation_range=20,\n",
        "                                             shear_range=0.2,\n",
        "                                             zoom_range=0.2,\n",
        "                                             width_shift_range=0.2,\n",
        "                                             height_shift_range=0.2,\n",
        "                                             horizontal_flip=True,\n",
        "                                             vertical_flip=True)\n",
        "                                            # etc.\n",
        "                                            # detail: https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator\n"
      ],
      "metadata": {
        "id": "dSv1_nvvBPO4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# alternatively, if you aren't using ImageDataGenerator, you can build in a data augmentation layer\n",
        "from tensorflow.keras.layers.experimental import preprocessing\n",
        "\n",
        "data_augmentation = tf.keras.models.Sequential([\n",
        "    preprocessing.RandomFlip(\"horizontal\"),\n",
        "    preprocessing.RandomHeight(0.2),\n",
        "    preprocessing.RandomWidth(0.2),\n",
        "    preprocessing.RandomZoom(0.2),\n",
        "    preprocessing.RandomRotation(0.2)\n",
        "    # preprocessing.Rescaling(1./255) # needed for ResNet, not EfficientNet\n",
        "],name=\"data_augmentation\")\n",
        "\n",
        "# may need tf.expand_dims(img,axis=0) to input a single image to data_augmentation"
      ],
      "metadata": {
        "id": "f_4l5oySCluB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### defining convolutional neural networks with Conv2D and pooling layers"
      ],
      "metadata": {
        "id": "jjM8hMed2KSq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define convolutional neural networks with conv2D and pooling layers\n",
        "# (binary)\n",
        "\n",
        "model = tf.keras.models.Sequential([\n",
        "    layers.Conv2D(filters=10,\n",
        "                  kernel_size=3,\n",
        "                  activation=\"relu\",\n",
        "                  input_shape=(224,224,3)), # see chosen target_size above\n",
        "    layers.Conv2D(10,3,activation=\"relu\"),\n",
        "    layers.MaxPool2D(pool_size=2,padding=\"valid\"), # padding could also be \"same\"\n",
        "    layers.Conv2D(10,3,activation=\"relu\"),\n",
        "    layers.Conv2D(10,3,activation=\"relu\"),\n",
        "    layers.MaxPool2D(2),\n",
        "    layers.Flatten(),\n",
        "    # layers.Dense(100,activation=\"relu\"), # could try this to improve performance...\n",
        "    layers.Dense(1,activation=\"sigmoid\")\n",
        "],name=\"model\")\n",
        "\n",
        "# looks like ImageDataGenerator gives you one-hot labels...\n",
        "# it also batches for you.\n",
        "model.compile(loss=\"binary_crossentropy\",\n",
        "              optimizer=tf.keras.optimizers.Adam(),\n",
        "              metrics=[\"accuracy\"])\n",
        "\n",
        "model.fit(train_data,\n",
        "          epochs=5,\n",
        "          steps_per_epoch=len(train_data),\n",
        "          validation_data=val_data,\n",
        "          validation_steps=len(val_data),\n",
        "          callbacks=[])"
      ],
      "metadata": {
        "id": "ZaXuvLS94NdM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for a multiclass problem, just change model output layer\n",
        "layers.Dense(NUM_CLASSES,activation=\"softmax\")\n",
        "\n",
        "# and change the compiling loss\n",
        "loss=\"categorical_crossentropy\"\n"
      ],
      "metadata": {
        "id": "rtWZLXsV6e4G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# a more simple conv2D and pooling model might be:\n",
        "# (still binary)\n",
        "\n",
        "model = tf.keras.models.Sequential([\n",
        "    layers.Conv2D(10,3,activation=\"relu\",input_shape=(224,224,3)),\n",
        "    layers.MaxPool2D(pool_size=2), # reduces the number of features by half\n",
        "    layers.Conv2D(10,3,activation=\"relu\"),\n",
        "    layers.MaxPool2D(),\n",
        "    layers.Conv2D(10,3,activation=\"relu\"),\n",
        "    layers.MaxPool2D(),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(1,activation=\"sigmoid\")\n",
        "])\n",
        "\n",
        "# with identical compiling and fitting protocols\n"
      ],
      "metadata": {
        "id": "GWQUMtyl1gnT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# understand how to use convolutions to improve your neural network\n",
        "\"\"\"\n",
        "convolution layers extract/learn the most important features from target images.\n",
        "pooling layers reduce the dimensionality of learned image features."
      ],
      "metadata": {
        "id": "P-jTEvIVBHGh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### transfer learning"
      ],
      "metadata": {
        "id": "llV8Oe4j78XQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# use pretrained models (transfer learning)\n",
        "\n",
        "# accessing pretrained models:\n",
        "# Resnet 50 V2 feature vector\n",
        "resnet_url = \"https://tfhub.dev/google/imagenet/resnet_v2_50/feature_vector/4\"\n",
        "\n",
        "# Original: EfficientNetB0 feature vector (version 1)\n",
        "efficientnet_url = \"https://tfhub.dev/tensorflow/efficientnet/b0/feature-vector/1\"\n"
      ],
      "metadata": {
        "id": "3CRm18jI3c1n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow_hub as hub\n",
        "\n",
        "feature_extractor_layer = hub.KerasLayer(model_url,\n",
        "                                         input_shape=(224,224)+(3,),\n",
        "                                         trainable=False, # to use pretrained weights\n",
        "                                         name=\"feature_extractor_layer\")\n",
        "\n",
        "model = tf.keras.models.Sequential([\n",
        "    feature_extractor_layer,\n",
        "    layers.Dense(num_classes,activation=\"softmax\")\n",
        "],name=\"model\")\n",
        "\n",
        "# compile and fit as usual, with loss=\"categorical_crossentropy\"\n"
      ],
      "metadata": {
        "id": "u9Z-5r8I8EG7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# or, use the functional API\n",
        "base_model = tf.keras.applications.EfficientNetB0(include_top=False)\n",
        "base_model.trainable = False\n",
        "\n",
        "inputs = layers.Input(shape=(224,224,3))\n",
        "# if using ResNet, need x = layers.experimental.preprocessing.Rescaling(1./255)(inputs)\n",
        "x = base_model(inputs)\n",
        "x = layers.GlobalAveragePooling2D()(x) # transforms a 4D tensor to a 2D tensor by averaging the values across the inner axes\n",
        "outputs = layers.Dense(num_classes,activation=\"softmax\")(x)\n",
        "model = tf.keras.Model(inputs,outputs)\n",
        "\n",
        "# if you add an augmentation layer between Input() and base_model(),\n",
        "# you must use x = base_model(x,training=False)\n",
        "\n",
        "# compile and fit as usual, with loss=\"categorical_crossentropy\"\n"
      ],
      "metadata": {
        "id": "Ni3RrwuF-ztl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# extract features from pre-trained models\n",
        "for layer_number,layer in enumerate(base_model.layers):\n",
        "  print(layer_number, layer.name)\n",
        "\n",
        "# can access weights by indexing on layers\n",
        "weights,biases = base_model.layers[1].get_weights()\n",
        "\n",
        "# input tensor shape with the same number of dimensions as the output of EfficientNetB0\n",
        "input_shape = (1,4,4,3)\n",
        "\n",
        "# create a random tensor\n",
        "input_tensor = tf.random.normal(input_shape)\n",
        "\n",
        "# pass the random tensor through a global average pooling 2D layer\n",
        "global_average_pooled_tensor = layers.GlobalAveragePooling2D()(input_tensor)\n",
        "\n",
        "# this is the same as tf.reduce_mean(input_tensor,axis=[1,2]) (averaging across the middle axes)\n"
      ],
      "metadata": {
        "id": "k3jyi4OS3fMR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### transfer learning fine-tuning"
      ],
      "metadata": {
        "id": "luJuqps09Y9b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# make sure you track history from the initial fit\n",
        "\n",
        "for layer_number, layer in enumerate(base_model.layers):\n",
        "  print(layer_number,layer.name,layer.trainable)\n",
        "\n",
        "# to fine-tune,\n",
        "base_model.trainable = True\n",
        "NUM_LAYERS_UNFROZEN = 10\n",
        "\n",
        "for layer in base_model.layers[:-NUM_LAYERS_UNFROZEN]:\n",
        "  layer.trainable = False\n",
        "\n",
        "# recompile\n",
        "model.compile(loss=\"categorical_crossentropy\",\n",
        "              optimizer=tf.keras.optimizers.Adam(lr=0.0001), # reduce lr by factor of 10\n",
        "              metrics=[\"accuracy\"])\n",
        "\n",
        "# refit model\n",
        "initial_epochs = 5\n",
        "fine_tune_epochs = initial_epochs + 5\n",
        "model.fit(train_data,\n",
        "          epochs=fine_tune_epochs,\n",
        "          validation_data=test_data,\n",
        "          initial_epoch=history.epoch[-1],\n",
        "          validation_steps=len(test_data),\n",
        "          callbacks=[])\n"
      ],
      "metadata": {
        "id": "5LHWYro59Wer"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Natural language processing (NLP)\n",
        "\n",
        "steps:\n",
        "1. **read in data**\n",
        "2. **turn train and test and/or val data into ```pd.DataFrame```(s)**\n",
        "3. **shuffle dataframe contents**\n",
        "4. **turn dataframe text into list(s)**\n",
        "5. if binary:\n",
        "    * turn dataframe labels ```.tolist()```\n",
        "\n",
        "  if multiclass:\n",
        "    * transform labels with ```OneHotEncoder```\n",
        "    * transform labels with ```LabelEncoder```\n",
        "    * find ```class_names``` and ```num_classes```\n",
        "\n",
        "6. **create tokenization of text**\n",
        "7. **create embedding for text**\n",
        "8. **design model using these layers**\n",
        "\n",
        "  if binary:\n",
        "    * output ```activation=\"sigmoid\"```, final shape is 1\n",
        "\n",
        "  if multiclass:\n",
        "    * output ```activation=\"softmax\"```, final shape is ```num_classes```\n",
        "\n",
        "9. **compile**\n",
        "\n",
        "  if binary:\n",
        "    * use ```binary_crossentropy``` and \"accuracy\" metric\n",
        "\n",
        "  if multiclass:\n",
        "    * use ```categorical_crossentropy``` (if using one-hot labels) and \"accuracy\" metric, or\n",
        "    * use ```sparse_categorical_crossentropy``` (if using integer labels)\n",
        "\n",
        "10. **fit model**\n",
        "\n",
        "  if multiclass:\n",
        "    * first use ```tf.data.Dataset.from_tensor_slices((text_list, labels_one_hot))``` and ```new_dataset.batch(BATCH_NUM).prefetch(tf.data.AUTOTUNE)``` to streamline fit\n",
        "\n",
        "11. **reload stored best weights for model**\n",
        "12. **evaluate**\n",
        "13. **predict**\n",
        "\n",
        "  if binary:\n",
        "    * ```model_pred_probs = model.predict(test_sentences)``` and ```model_preds = tf.squeeze(tf.round(model_pred_probs))```\n",
        "\n",
        "  if multiclass:\n",
        "    * ```model_pred_probs = model.predict(test_dataset)``` and ```model_preds = tf.argmax(model_pred_probs,axis=1)```\n",
        "    * in comparisons of preds and true labels, use ```test_labels_encoded```"
      ],
      "metadata": {
        "id": "gGA3rUCXBaBh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# build natural language processing systems using tensorflow\n"
      ],
      "metadata": {
        "id": "eDh7A1gWBdde"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### getting NLP data (reading txt document lines)"
      ],
      "metadata": {
        "id": "ZJjU2HheS6D7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/Franck-Dernoncourt/pubmed-rct\n",
        "!ls pubmed-rct\n",
        "\n",
        "# check what files are in the 20k dataset\n",
        "!ls pubmed-rct/PubMed_20k_RCT_numbers_replaced_with_at_sign/\n",
        "\n",
        "# start our experiments using the 20k dataset with numbers replaced by @\n",
        "data_dir = \"/content/pubmed-rct/PubMed_20k_RCT_numbers_replaced_with_at_sign/\"\n",
        "\n",
        "# check all of the filenames in the target directory\n",
        "import os\n",
        "filenames = [data_dir+filename for filename in os.listdir(data_dir)]\n",
        "filenames"
      ],
      "metadata": {
        "id": "OCWlF0Vj4_Di"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def get_lines(filename):\n",
        "#   with open(filename, \"r\") as f:\n",
        "#       return f.readlines()\n",
        "\n",
        "f = open(filename,\"r\")\n",
        "lines = f.readlines()\n",
        "\n",
        "# create a dictionary containing details like \"line_number\", \"target\", \"text\"\n",
        "\n",
        "# # you can use line parsing tools like:\n",
        "# for line in input_lines:\n",
        "#   if line.startswith(\"###\"):\n",
        "#   elif line.isspace():\n",
        "\n",
        "# abstract_line_split = abstract_lines.splitlines() # split text \n",
        "\n",
        "# for abstract_line_number,abstract_line in enumerate(abstract_line_split)\n",
        "# target_text_split = abstract_line.split(\"\\t\")\n",
        "# target_text_split[0].lower()\n",
        "\n",
        "# # this is in 09. milestone project \n",
        "# train_samples = preprocess_text_with_line_numbers(data_dir + \"train.txt\")\n",
        "# val_samples = preprocess_text_with_line_numbers(data_dir + \"dev.txt\") # dev is another name for validation set\n",
        "# test_samples = preprocess_text_with_line_numbers(data_dir + \"test.txt\")\n"
      ],
      "metadata": {
        "id": "eUlr6XS3S-SX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create a dictionaries train_samples, val_samples, and test_samples\n",
        "# then, make them into pandas dataframes:\n",
        "train_df = pd.DataFrame(train_samples)\n",
        "val_df = pd.DataFrame(val_samples)\n",
        "test_df = pd.DataFrame(test_samples)\n",
        "\n",
        "train_sentences = train_df[\"text\"].tolist()\n",
        "val_sentences = val_df[\"text\"].tolist()\n",
        "test_sentences = test_df[\"text\"].tolist()\n"
      ],
      "metadata": {
        "id": "Vj8qjOLt5lg3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### making numeric labels: one-hot encoding and label encoding of labels (multiclass categorization)"
      ],
      "metadata": {
        "id": "jR7MAlFkdBj3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# extract labels and one-hot encode them\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "one_hot_encoder = OneHotEncoder(sparse=False)\n",
        "train_labels_one_hot = one_hot_encoder.fit_transform(train_df[\"target\"].to_numpy().reshape(-1,1))\n",
        "val_labels_one_hot = one_hot_encoder.fit_transform(val_df[\"target\"].to_numpy().reshape(-1,1))\n",
        "test_labels_one_hot = one_hot_encoder.fit_transform(test_df[\"target\"].to_numpy().reshape(-1,1))\n"
      ],
      "metadata": {
        "id": "hPCmuRfedG7N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# extract labels and integer encode them\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "label_encoder = LabelEncoder()\n",
        "train_labels_encoded = label_encoder.fit_transform(train_df[\"target\"].to_numpy())\n",
        "val_labels_encoded = label_encoder.fit_transform(val_df[\"target\"].to_numpy())\n",
        "test_labels_encoded = label_encoder.fit_transform(test_df[\"target\"].to_numpy())\n"
      ],
      "metadata": {
        "id": "3inDIjEM6uJU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# you can get class names if you transform with LabelEncoder!!!\n",
        "class_names = label_encoder.classes_\n",
        "num_classes = len(class_names)\n"
      ],
      "metadata": {
        "id": "bsljoomp7J6R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### getting NLP data (download and unzip csv)"
      ],
      "metadata": {
        "id": "Gj-_uYNOHRvT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prepare text to use in tensorflow models\n",
        "# Download data (same as from Kaggle)\n",
        "!wget \"https://storage.googleapis.com/ztm_tf_course/nlp_getting_started.zip\"\n",
        "\n",
        "import zipfile\n",
        "\n",
        "# Unzip data\n",
        "zip_ref = zipfile.ZipFile(\"nlp_getting_started.zip\", \"r\")\n",
        "zip_ref.extractall()\n",
        "zip_ref.close()\n",
        "\n",
        "# Turn .csv files into pandas DataFrames\n",
        "import pandas as pd\n",
        "train_df = pd.read_csv(\"train.csv\")\n",
        "test_df = pd.read_csv(\"test.csv\")\n",
        "train_df.head()\n"
      ],
      "metadata": {
        "id": "GddpYRuyBhDg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### shuffling a training dataframe"
      ],
      "metadata": {
        "id": "d5qbt_BTHwqe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Shuffle training dataframe\n",
        "train_df_shuffled = train_df.sample(frac=1, random_state=42) # shuffle with random_state=42 for reproducibility\n"
      ],
      "metadata": {
        "id": "hNYV2j05HyvR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### creating NLP train/val sets with train_test_split"
      ],
      "metadata": {
        "id": "Z4r6PJIFIKmM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Use train_test_split to split training data into training and validation sets\n",
        "train_sentences, val_sentences, train_labels, val_labels = train_test_split(train_df_shuffled[\"text\"].to_numpy(),\n",
        "                                                                            train_df_shuffled[\"target\"].to_numpy(),\n",
        "                                                                            test_size=0.1, # dedicate 10% of samples to validation set\n",
        "                                                                            random_state=42) # random state for reproducibility\n",
        "                                                                            "
      ],
      "metadata": {
        "id": "etIVJNU1IBp0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TensorFlow Hub USE encoding"
      ],
      "metadata": {
        "id": "u_7HxD8ZRJee"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow_hub as hub\n",
        "\n",
        "sentence_encoder_layer = hub.KerasLayer(\"https://tfhub.dev/google/universal-sentence-encoder/4\",\n",
        "                                        input_shape=[], # shape of inputs coming to our model \n",
        "                                        dtype=tf.string, # data type of inputs coming to the USE layer\n",
        "                                        trainable=False, # keep the pretrained weights (we'll create a feature extractor)\n",
        "                                        name=\"USE\")\n",
        "\n",
        "# you can instead fine-tune the TF Hub USE by setting trainable=True\n",
        "\n",
        "# to be used specially in a model:\n",
        "model = tf.keras.models.Sequential([\n",
        "    sentence_encoder_layer,\n",
        "    layers.Dense(64,activation=\"relu\"),\n",
        "    layers.Dense(1,activation=\"sigmoid\")\n",
        "],name=\"model\")\n",
        "\n",
        "# compile and fit as below (\"building a binary model\")\n"
      ],
      "metadata": {
        "id": "YkFZPdBPRNqK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### tokenization options"
      ],
      "metadata": {
        "id": "qdjXgWtqIeL2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# use word embeddings in your tensorflow model\n"
      ],
      "metadata": {
        "id": "g3KHXkZBBwNc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
        "# Note: in TensorFlow 2.6+, you no longer need \"layers.experimental.preprocessing\"\n",
        "# you can use: \"tf.keras.layers.TextVectorization\", see https://github.com/tensorflow/tensorflow/releases/tag/v2.6.0 for more\n",
        "\n",
        "# for max_tokens, either multiples of 10000 or exact number of unique words in your text\n",
        "max_vocab_length = 10000 # max number of words to have in our vocabulary\n",
        "\n",
        "# average number of tokens per unit\n",
        "# something like: round(sum([len(i.split()) for i in train_sentences])/len(train_sentences))\n",
        "max_length = 15 # max length our sequences will be (e.g. how many words from a Tweet does our model see?)\n"
      ],
      "metadata": {
        "id": "v4Aw6V80IqC6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenization layer: binary example\n",
        "text_vectorizer = TextVectorization(max_tokens=max_vocab_length,\n",
        "                                    output_mode=\"int\",\n",
        "                                    output_sequence_length=max_length)\n",
        "\n",
        "text_vectorizer.adapt(train_sentences)\n"
      ],
      "metadata": {
        "id": "IcOCbBRJ-pMR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenization layer: multiclass example (token)\n",
        "text_vectorizer = TextVectorization(max_tokens=max_vocab_length,\n",
        "                                    output_sequence_length=max_length)\n",
        "\n",
        "text_vectorizer.adapt(train_sentences)\n",
        "\n",
        "# max_length for sentences may be found as e.g. int(np.percentile(sent_lens,95))"
      ],
      "metadata": {
        "id": "ofh3ndtk-pgY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenization layer: multiclass example (char)\n",
        "\n",
        "def split_chars(text):\n",
        "  return \" \".join(list(text))\n",
        "\n",
        "train_chars = [split_chars(sentence) for sentence in train_sentences]\n",
        "val_chars = [split_chars(sentence) for sentence in val_sentences]\n",
        "test_chars = [split_chars(sentence) for sentence in test_sentences]\n",
        "\n",
        "import string\n",
        "alphabet = string.ascii_lowercase + string.digits + string.punctuation\n",
        "\n",
        "num_char_tokens = len(alphabet)+2 (to include space and OOV)\n",
        "# max_length for chars may be found as e.g. int(np.percentile(char_lens,95))\n",
        "\n",
        "char_vectorizer = TextVectorization(max_tokens=num_char_tokens,\n",
        "                                    output_sequence_length=max_length,\n",
        "                                    standardize=\"lower_and_strip_punctuation\")\n",
        "\n",
        "char_vectorizer.adapt(train_chars)\n"
      ],
      "metadata": {
        "id": "vjiH5HdG-sUD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### embedding options"
      ],
      "metadata": {
        "id": "rzscN0SkJrgx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import layers\n",
        "\n",
        "# embedding layer: binary example\n",
        "embedding = layers.Embedding(input_dim=max_vocab_length, # set input shape\n",
        "                             output_dim=128, # set size of embedding vector\n",
        "                             embeddings_initializer=\"uniform\", # default, intialize randomly\n",
        "                             input_length=max_length, # how long is each input\n",
        "                             name=\"embedding\") \n"
      ],
      "metadata": {
        "id": "Ra1A_NNeJiUQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# embedding layer: multiclass example (token)\n",
        "embedding = layers.Embedding(input_dim=max_vocab_length, # set input shape\n",
        "                             output_dim=128, # set size of embedding vector\n",
        "                             # use masking to handle variable sequence length\n",
        "                             mask_zero=True,\n",
        "                             name=\"embedding\")\n"
      ],
      "metadata": {
        "id": "cnVaduSR-PT3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# embedding layer: multiclass example (char)\n",
        "char_embed = layers.Embedding(input_dim=num_char_tokens,\n",
        "                              output_dim=25,\n",
        "                              mask_zero=False, # DON'T use masks\n",
        "                              name=\"embedding\")\n"
      ],
      "metadata": {
        "id": "l1I4sdXx-fKJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### hybrid embedding (multiclass categorization)\n",
        "\n",
        "e.g. take token and character-level sequences as input and produce sequence label probabilities as output"
      ],
      "metadata": {
        "id": "A_WtjO5yMMvr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# set up token inputs/model (e.g. USE tokenization/embedding)\n",
        "token_inputs = layers.Input(shape=[],dtype=\"string\")\n",
        "x = tf_hub_embedding_layer(token_inputs)\n",
        "token_outputs = layers.Dense(128,activation=\"relu\")(x)\n",
        "token_model = tf.keras.Model(token_inputs,token_outputs,name=\"token_model\")\n",
        "\n",
        "# set up char inputs/model (e.g. Bidirectional LSTM)\n",
        "char_inputs = layers.Input(shape=(1,),dtype=\"string\")\n",
        "x = char_vectorizer(char_inputs)\n",
        "x = char_embed(x)\n",
        "char_outputs = layers.Bidirectional(layers.LSTM(25))(x)\n",
        "char_model = tf.keras.Model(char_inputs,char_outputs,name=\"char_model\")\n",
        "\n",
        "# concatenate token and char inputs\n",
        "token_char_concat = layers.Concatenate()([token_model.output,\n",
        "                                          char_model.output])\n",
        "\n",
        "# create output layers (including Dropout)\n",
        "combined_dropout = layers.Dropout(0.5)(token_char_concat)\n",
        "combined_dense = layers.Dense(200,activation=\"relu\")(combined_dropout)\n",
        "final_dropout = layers.Dropout(0.5)(combined_dense)\n",
        "output_layer = layers.Dense(num_classes)\n",
        "\n",
        "# construct model with char and token inputs\n",
        "model = tf.keras.Model(inputs=[token_model.input,\n",
        "                               char_model.input],\n",
        "                       outputs=output_layer,\n",
        "                       name=\"model\")\n",
        "\n",
        "# model.summary()\n",
        "# from tensorflow.keras.utils import plot_model\n",
        "# plot_model(model)\n",
        "\n",
        "# compile as normal\n",
        "# could use loss=tf.keras.losses.CategoricalCrossentropy(label_smoothing=0.2)\n"
      ],
      "metadata": {
        "id": "rGSWpr3iMOwO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_char_token_data = tf.data.Dataset.from_tensor_slices((train_sentences,train_chars))\n",
        "train_char_token_labels = tf.data.Dataset.from_tensor_slices(train_labels_one_hot)\n",
        "train_char_token_dataset = tf.data.Dataset.zip((train_char_token_data,train_char_token_labels))\n",
        "train_char_token_dataset = train_char_token_dataset.batch(32).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "# do the same for val_char_token_dataset and test_char_token_dataset\n",
        "\n",
        "model.fit(train_char_token_dataset,\n",
        "          epochs=3,\n",
        "          steps_per_epoch=len(train_char_token_dataset),\n",
        "          validation_data=val_char_token_dataset,\n",
        "          validation_steps=len(val_char_token_dataset),\n",
        "          callbacks=[])\n"
      ],
      "metadata": {
        "id": "KS_b5AlHPN2E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### building a binary model"
      ],
      "metadata": {
        "id": "2IjcUSfIKK-M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# build models that identify the category of a piece of text using\n",
        "# binary categorization\n"
      ],
      "metadata": {
        "id": "nFv_IGVYBjFj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import layers\n",
        "inputs = layers.Input(shape=(1,),dtype=\"string\") # inputs are 1D strings\n",
        "x = text_vectorizer(inputs)\n",
        "x = embedding(x)\n",
        "x = layers.GlobalAveragePooling1D()(x)\n",
        "outputs = layers.Dense(1,activation=\"sigmoid\")(x) # binary output\n",
        "model = tf.keras.Model(inputs,outputs,name=\"model\")\n",
        "\n",
        "model.compile(loss=\"binary_crossentropy\",\n",
        "              optimizer=tf.keras.optimizers.Adam(),\n",
        "              metrics=[\"accuracy\"])\n",
        "\n",
        "model.fit(x=train_sentences,\n",
        "          y=train_labels,\n",
        "          epochs=5,\n",
        "          validation_data=(val_sentences,val_labels),\n",
        "          callbacks=[])\n",
        "\n",
        "# can check embedding weights: embedding.weights\n",
        "# embed_weights = model.get_layer(\"embedding\").get_weights()[0]\n",
        "\n",
        "# you get a binary prediction from rounding the output number\n",
        "model_pred_probs = model.predict(val_sentences)\n",
        "model_preds = tf.squeeze(tf.round(model_pred_probs))\n"
      ],
      "metadata": {
        "id": "y0d7lwOdKMcW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### building a multiclass model"
      ],
      "metadata": {
        "id": "Fm6XzeCcAnYn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# build models that identify the category of a piece of text using\n",
        "# multiclass categorization\n",
        "\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((train_sentences,train_labels_one_hot))\n",
        "train_dataset = train_dataset.batch(32).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "val_dataset = tf.data.Dataset.from_tensor_slices((val_sentences,val_labels_one_hot))\n",
        "val_dataset = val_dataset.batch(32).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((test_sentences,test_labels_one_hot))\n",
        "test_dataset = test_dataset.batch(32).prefetch(tf.data.AUTOTUNE)\n"
      ],
      "metadata": {
        "id": "Cy2iSqE6Bo1B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import layers\n",
        "inputs = layers.Input(shape=(1,),dtype=\"string\")\n",
        "x = text_vectorizer(inputs)\n",
        "x = embeddings(x)\n",
        "x = layers.Conv1D(64,kernel_size=5,padding=\"same\",activation=\"relu\")(x)\n",
        "x = layers.GlobalAveragePooling1D()(x)\n",
        "outputs = layers.Dense(num_classes,activiation=\"softmax\")(x)\n",
        "model = tf.keras.Model(inputs,outputs,name=\"model\")\n",
        "\n",
        "model.compile(loss=\"categorical_crossentropy\",\n",
        "              optimizer=tf.keras.optimizers.Adam(),\n",
        "              metrics=[\"accuracy\"])\n",
        "\n",
        "model.fit(train_dataset,\n",
        "          epochs=3,\n",
        "          steps_per_epoch=len(train_dataset),\n",
        "          validation_data=val_dataset,\n",
        "          validation_steps=len(val_dataset),\n",
        "          callbacks=[])\n",
        "\n",
        "# you get a categorical prediction from argmax\n",
        "model_pred_probs = model.predict(test_dataset)\n",
        "model_preds = tf.argmax(model_pred_probs,axis=1)\n",
        "\n",
        "# see char equivalent in \n"
      ],
      "metadata": {
        "id": "YSVsQ2apApm7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### using an LSTM in an NLP model\n",
        "\n",
        "can also make bidirectional by nesting layers:\n",
        "```\n",
        "x = layers.Bidirectional(layers.LSTM(64))(x)\n",
        "```"
      ],
      "metadata": {
        "id": "obQ9lIz6OzNe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# use LSTMs in your model to classify text for either binary or \n",
        "# multiclass categorization\n",
        "\n",
        "# binary LSTM:\n",
        "inputs = layers.Input(shape=(1,),dtype=\"string\")\n",
        "x = text_vectorizer(inputs)\n",
        "x = embedding(x)\n",
        "# if stacking RNN/LSTM layers, \n",
        "# x = layers.LSTM(64,return_sequences=True)(x) # to return vector for each word\n",
        "x = layers.LSTM(64)(x) # return vector for whole sequence\n",
        "# could add: x = layers.Dense(64,activation=\"relu\")(x)\n",
        "outputs = layers.Dense(1,activation=\"sigmoid\")(x)\n",
        "model = tf.keras.Model(inputs,outputs,name=\"model\")\n",
        "\n",
        "# compile and fit as above (\"building a binary model\")\n"
      ],
      "metadata": {
        "id": "qOqFkPLvByfK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### using a GRU in an NLP model\n",
        "\n",
        "can also make bidirectional by nesting layers:\n",
        "```\n",
        "x = layers.Bidirectional(layers.GRU(64))(x)\n",
        "```"
      ],
      "metadata": {
        "id": "cvLnvI_QOebn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# add RNN and GRU layers to your model\n",
        "\n",
        "# binary GRU\n",
        "inputs = layers.Input(shape=(1,),dtype=\"string\")\n",
        "x = text_vectorizer(inputs)\n",
        "x = embedding(x)\n",
        "# if stacking RNN/GRU layers, \n",
        "# x = layers.GRU(64,return_sequences=True)(x) # to return vector for each word\n",
        "x = layers.GRU(64)(x) # return vector for whole sequence\n",
        "# could add: x = layers.Dense(64,activation=\"relu\")(x)\n",
        "outputs = layers.Dense(1,activation=\"sigmoid\")(x)\n",
        "model = tf.keras.Model(inputs,outputs,name=\"model\")\n",
        "\n",
        "# compile and fit as above (\"building a binary model\")\n"
      ],
      "metadata": {
        "id": "o0SPuU3AB3ur"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### using a CNN in an NLP model"
      ],
      "metadata": {
        "id": "RlJmo24NPaVP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# use RNNs, LSTMs, GRUs, and CNNs in models that work with text\n",
        "inputs = layers.Input(shape=(1,),dtype=\"string\")\n",
        "x = text_vectorizer(inputs)\n",
        "x = embedding(x)\n",
        "x = layers.Conv1D(filters=32,kernel_size=5,activation=\"relu\")(x)\n",
        "x = layers.GlobalMaxPool1D()(x)\n",
        "# could add: x = layers.Dense(64,activation=\"relu\")(x)\n",
        "outputs = layers.Dense(1,activation=\"sigmoid\")(x)\n",
        "model = tf.keras.Model(inputs,outputs,name=\"model\")\n",
        "\n",
        "# compile and fit as above (\"building a binary model\")\n",
        "\n",
        "# for a multiclass model, see above (\"building a multiclass model\")\n"
      ],
      "metadata": {
        "id": "ggNjKuzFB6Bk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### text generation\n",
        "\n",
        "https://www.tensorflow.org/text/tutorials/text_generation"
      ],
      "metadata": {
        "id": "t1fnEY5R9ULW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# train LSTMs on existing text to generate text (such as songs and poetry)\n",
        "\n",
        "# see other colab notebook: text_generation\n"
      ],
      "metadata": {
        "id": "69smx53qB-GO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Time series, sequences, and predictions\n",
        "\n",
        "steps:\n",
        "1. **read in data**\n",
        "2. **turn data into ```pd.DataFrame```**\n",
        "3. **DO NOT shuffle dataframe contents**\n",
        "4. **turn dataframe content into lists**\n",
        "    * create ```X``` as price windows list\n",
        "    * create ```y``` as corresponding price list\n",
        "    * if multivariate, make appropriate n other lists\n",
        "\n",
        "  list creation approach options include:\n",
        "    * windowed array approach\n",
        "    * copy of ```pd.Dataframe``` and addition of columns defining shifting windows\n",
        "\n",
        "5. **split into train and test sets with a percentage-based split size**\n",
        "    * earlier datapoints must constitute the train dataset\n",
        "    * later datapoints must constitute the test dataset\n",
        "\n",
        "6. **design model**\n",
        "    * output layer: ```layers.Dense(HORIZON,activation=\"linear\")```\n",
        "\n",
        "7. **compile**\n",
        "    * use ```mae``` for loss\n",
        "\n",
        "8. **fit model**\n",
        "    * first use ```tf.data.Dataset.from_tensor_slices((windows, prices))``` and ```new_dataset.batch(BATCH_NUM).prefetch(tf.data.AUTOTUNE)``` to create fast-loading batched datasets\n",
        "\n",
        "9. **reload stored best weights for model**\n",
        "10. **evaluate**\n",
        "11. **predict**\n",
        "    * ```model_preds = model.predict(test_windows)```\n",
        "    * for predictions with horizons > 1, ```model_preds = tf.squeeze(model_preds)```\n",
        "\n",
        "12. **forecast**\n",
        "    * use iterations of model training, prediction, and dataset update for however many future steps you'd like\n",
        "    * you might try first smoothing data with a trailing window of some width (perhaps beginning with 3?)"
      ],
      "metadata": {
        "id": "MGWeSN-nCDPH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# train, tune, and use time series, sequence, and prediction models\n"
      ],
      "metadata": {
        "id": "3YH-AJwMCCfR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### getting time series data with pd"
      ],
      "metadata": {
        "id": "LsYGAVpAAUwT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prepare data for time series learning\n"
      ],
      "metadata": {
        "id": "-ejfc24GCNn5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# preprocess data to get it ready for use in a model\n",
        "\n",
        "# data from GitHub:\n",
        "# Note: you'll need to select \"Raw\" to download the data in the correct format\n",
        "!wget https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/extras/BTC_USD_2013-10-01_2021-05-18-CoinDesk.csv \n",
        "\n",
        "# time series with pandas:\n",
        "import pandas as pd\n",
        "df = pd.read_csv(\"/content/BTC_USD_2013-10-01_2021-05-18-CoinDesk.csv\", \n",
        "                 parse_dates=[\"Date\"], \n",
        "                 index_col=[\"Date\"]) # parse the date column (tell pandas column 1 is a datetime)\n",
        "\n",
        "bitcoin_prices = pd.DataFrame(df[\"Closing Price (USD)\"]).rename(columns={\"Closing Price (USD)\": \"Price\"})\n"
      ],
      "metadata": {
        "id": "J82wLP1e3Fo4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "timesteps = bitcoin_prices.index.to_numpy()\n",
        "prices = bitcoin_prices[\"Price\"].to_numpy()\n"
      ],
      "metadata": {
        "id": "slwrxe5NBCmW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### time series train/test sets with price list (univariate)\n"
      ],
      "metadata": {
        "id": "gLZ9s0UOCSFF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prepare features and labels\n"
      ],
      "metadata": {
        "id": "cl7MtLZGCfoM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "WINDOW_SIZE = 7\n",
        "HORIZON = 1\n",
        "\n",
        "# create a window of specific window_size\n",
        "window_step = np.expand_dims(np.arange(WINDOW_SIZE+HORIZON),axis=0)\n",
        "\n",
        "# create a 2D array of multiple window steps\n",
        "window_indices = window_step + np.expand_dims(np.arange(len(prices)-(WINDOW_SIZE+HORIZON-1)),axis=0).T\n",
        "\n",
        "# index on the time series with 2D array\n",
        "windowed_array = prices[window_indices]\n",
        "\n",
        "# finally, label windowed data\n",
        "full_windows, full_labels = windowed_array[:,:-HORIZON], prices[:,-HORIZON:]\n",
        "\n",
        "# need train data to be early data and test data to be later data\n",
        "split_size = int(0.8*len(prices)) # 80% train, 20% test\n",
        "\n",
        "train_windows, train_labels = full_windows[:split_size], full_labels[:split_size]\n",
        "test_windows, test_labels = full_windows[split_size:], full_labels[split_size:]\n"
      ],
      "metadata": {
        "id": "ul26b4gwCf2V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### fast-loading batched datasets with price list (univariate)"
      ],
      "metadata": {
        "id": "cMMsQUNlFeeL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_NUM = 128\n",
        "\n",
        "train_windows_data = tf.data.Dataset.from_tensor_slices(train_windows)\n",
        "train_labels_data = tf.data.Dataset.from_tensor_slices(train_labels)\n",
        "train_dataset = tf.data.Dataset.zip((train_windows_data,train_labels_data))\n",
        "train_dataset = train_dataset.batch(BATCH_NUM).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "test_windows_data = tf.data.Dataset.from_tensor_slices(test_windows)\n",
        "test_labels_data = tf.data.Dataset.from_tensor_slices(test_labels)\n",
        "test_dataset = tf.data.Dataset.zip((test_windows_data,test_labels_data))\n",
        "test_dataset = test_dataset.batch(BATCH_NUM).prefetch(tf.data.AUTOTUNE)\n"
      ],
      "metadata": {
        "id": "1recDQnyFiVy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### time series train/test sets with pd (univariate _or_ multivariate)"
      ],
      "metadata": {
        "id": "LXStAO2-ZNZu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bitcoin_prices_windowed = bitcoin_prices.copy()\n",
        "\n",
        "# for each \"Price\" and \"Block_Reward\" in bitcoin_prices, we need WINDOW_SIZE columns of values\n",
        "for i in range(WINDOW_SIZE):\n",
        "  bitcoin_prices_windowed[f\"Price+{i+1}\"] = bitcoin_prices_windowed[\"Price\"].shift(periods=i+1)\n",
        "\n",
        "split_size = int(len(y)*0.8)\n",
        "\n",
        "# need the labels...\n",
        "y = bitcoin_prices_windowed.dropna()[\"Price\"].astype(np.float32)\n",
        "\n",
        "###\n",
        "\n",
        "# to consider both variables as joint inputs to the same layer:\n",
        "X = bitcoin_prices_windowed.dropna().drop(\"Price\",axis=1).astype(np.float32) # (same as usual, just happens to have an extra column)\n",
        "X_train, y_train = X[:split_size], y[:split_size]\n",
        "X_test, y_test = X[split_size:], y[split_size:]\n",
        "\n",
        "# to consider variables as inputs to separate layers:\n",
        "X = bitcoin_prices_windowed.dropna().drop(\"Price\",axis=1).astype(np.float32)\n",
        "X_price_window = X.dropna().drop(\"Block_Reward\",axis=1).astype(np.float32)\n",
        "X_block_reward = X.dropna()[\"Block_Reward\"].astype(np.float32)\n",
        "\n",
        "X_price_window_train, X_block_reward_train, y_train = X_price_window[:split_size], X_block_reward[:split_size], y[:split_size]\n",
        "X_price_window_test, X_block_reward_test, y_test = X_price_window[split_size:], X_block_reward[split_size:], y[split_size:]\n",
        "\n",
        "###\n"
      ],
      "metadata": {
        "id": "sU5o8J9yZL_m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### fast-loading batched datasets with pd (univariate _or_ multivariate)"
      ],
      "metadata": {
        "id": "Si5TMc1Kbz0d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_NUM = 128\n",
        "\n",
        "y_train_data = tf.data.Dataset.from_tensor_slices(y_train)\n",
        "y_test_data = tf.data.Dataset.from_tensor_slices(y_test)\n",
        "\n",
        "###\n",
        "\n",
        "# to consider both variables as joint inputs to the same layer:\n",
        "X_train_data = tf.data.Dataset.from_tensor_slices(X_train)\n",
        "X_test_data = tf.data.Dataset.from_tensor_slices(X_test)\n",
        "\n",
        "# to consider variables as inputs to separate layers:\n",
        "X_train_data = tf.data.Dataset.from_tensor_slices((X_price_window_train,X_block_reward_train))\n",
        "X_test_data = tf.data.Dataset.from_tensor_slices((X_price_window_test,X_block_reward_test))\n",
        "\n",
        "###\n",
        "\n",
        "train_dataset = tf.data.Dataset.zip((X_train_data,y_train_data))\n",
        "train_dataset = train_dataset.batch(BATCH_NUM).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "test_dataset = tf.data.Dataset.zip((X_test_data,y_test_data))\n",
        "test_dataset = test_dataset.batch(BATCH_NUM).prefetch(tf.data.AUTOTUNE)\n"
      ],
      "metadata": {
        "id": "X76UXUvnbxaW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### multivariate time series models"
      ],
      "metadata": {
        "id": "l7w28jUP8RMi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = \n",
        "\n",
        "# to consider both variables as joint inputs to the same layer:\n",
        "\n",
        "# to consider variables as inputs to separate layers:\n",
        "\n",
        "price_window_model\n",
        "\n",
        "block_reward_model\n",
        "\n",
        "\n",
        "# ordinary compiling and fitting using train_dataset (and test_dataset)\n"
      ],
      "metadata": {
        "id": "b3i1nLiZ8V3W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### time series prediction"
      ],
      "metadata": {
        "id": "02MJUC4eHCin"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# train models to predict values for both univariate and multivariate\n",
        "# time series\n"
      ],
      "metadata": {
        "id": "NtuPETsDCK4q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# understand Mean Absolute Error (MAE) and how it can be used to\n",
        "# evaluate accuracy of sequence models\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    layers.Dense(128,activation=\"relu\"),\n",
        "    layers.Dense(HORIZON,activation=\"linear\") # activation=\"linear\" is unnecessary\n",
        "],name=\"model\")\n",
        "\n",
        "model.compile(loss=\"mae\",\n",
        "              optimizer=tf.keras.optimizers.Adam())\n",
        "\n",
        "model.fit(x=train_windows,\n",
        "          y=train_labels,\n",
        "          epochs=100,\n",
        "          batch_size=128,\n",
        "          validation_data=(test_windows,test_labels),\n",
        "          callbacks=[create_model_checkpoint(\"model\")]) # to save best model, see callback section below\n",
        "# -or-\n",
        "\n",
        "model.fit(train_dataset,\n",
        "          epochs=100,\n",
        "          steps_per_epoch=len(train_dataset),\n",
        "          validation_data=test_dataset,\n",
        "          validation_steps=len(test_dataset)\n",
        "          callbacks=[create_model_checkpoint(\"model\")]) # to save best model, see callback section below\n",
        "\n",
        "# reload best model\n",
        "model = tf.keras.models.load_model(\"model_experiments/model\")\n",
        "\n",
        "# assess model\n",
        "model.evaluate(test_windows,test_labels) # or model.evaluate(test_dataset)\n",
        "\n",
        "# predict with model\n",
        "model_preds = model.predict(test_windows) # or model.predict(test_dataset)\n",
        "\n",
        "# predictions with bigger horizons\n",
        "model_preds = tf.squeeze(model_preds)\n",
        "\n",
        "# if you have shape problems, try tf.squeeze(test_labels) when comparing\n",
        "# test_labels and model_preds\n",
        "\n",
        "# see \"what is MAE\" below for a way to evaluate bigger window bois\n"
      ],
      "metadata": {
        "id": "qQzTwXtiCStk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What is MAE?\n",
        "\n",
        "mean absolute error: forecast methods minimizing the MAE lead to forecasts of the median.\n",
        "\n",
        "```\n",
        "mae = tf.reduce_mean(tf.abs(y_true-y_pred))\n",
        "mae = tf.keras.metrics.mean_absolute_error(y_true,y_pred)\n",
        "\n",
        "if mae.ndim > 0:\n",
        "  mae = tf.reduce_mean(mae)\n",
        "\n",
        "mae.numpy()\n",
        "```\n",
        "\n",
        "RMSE: forecast methods minimizing the RMSE lead to forecasts of the mean.\n",
        "\n",
        "```\n",
        "mse = tf.keras.metrics.mean_squared_error(y_true,y_pred)\n",
        "rmse = tf.sqrt(mse)\n",
        "\n",
        "if mae.ndim > 0:\n",
        "  rmse = tf.reduce_mean(rmse)\n",
        "\n",
        "rmse.numpy()\n",
        "```"
      ],
      "metadata": {
        "id": "AmHxzQHdIoos"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### using a CNN in a time series problem"
      ],
      "metadata": {
        "id": "CP13CMm0N7AV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# use RNNs and CNNs for time series, sequence, and forecasting models\n",
        "\n",
        "# to make a CNN:\n",
        "# before we pass our data to the conv1D layer, we need to reshape it\n",
        "x = tf.constant(train_windows[0])\n",
        "\n",
        "model = tf.keras.models.Sequential([\n",
        "    layers.Lambda(lambda x: tf.expand_dims(x,axis=1)),\n",
        "    layers.Conv1D(filters=128,kernel_size=5,padding=\"causal\",activation=\"relu\"),\n",
        "    layers.Dense(HORIZON)\n",
        "],name=\"model\")\n",
        "\n",
        "# with ordinary compiling and fitting (see above, under \"time series prediction\")\n"
      ],
      "metadata": {
        "id": "uVpgfn8dCYJY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### using an RNN (LSTM) in a time series problem\n"
      ],
      "metadata": {
        "id": "V4MQ2jSSN8bD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# to make a RNN (LSTM):\n",
        "# we'll use the functional API\n",
        "x = tf.constant(train_windows[0])\n",
        "\n",
        "inputs = layers.Input(shape=(WINDOW_SIZE))\n",
        "x = layers.Lambda(lambda x: tf.expand_dims(x,axis=1))(inputs)\n",
        "x = layers.LSTM(128,activation=\"relu\")(x) # get a massive error with activation=\"tanh\"\n",
        "# could try adding dense layers to improve performance, \n",
        "# like x = layers.Dense(32,activation=\"relu\")(x)\n",
        "# in that case, layers.LSTM requires an additional return_sequences=True\n",
        "output = layers.Dense(HORIZON)(x)\n",
        "model = tf.keras.Model(inputs,outputs,name=\"model\")\n",
        "\n",
        "# with ordinary compiling and fitting (see above, under \"time series prediction\")\n"
      ],
      "metadata": {
        "id": "sGapoy60OA4r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### time series forecasting"
      ],
      "metadata": {
        "id": "rWFZGH3yO0vY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# identify when to use trailing versus centered windows\n",
        "\n",
        "# use centered windows when directionality doesn't matter\n",
        "# in time series, when doing forecasting, you probably need a trailing average\n",
        "\n",
        "# # i might institute this with something like...\n",
        "# bitcoin_prices[\"Moving_Average\"] = None\n",
        "\n",
        "# AVG_KERNEL = 3\n",
        "\n",
        "# # create a window of specific averaging kernel size\n",
        "# window_step = np.expand_dims(np.arange(AVG_KERNEL),axis=0)\n",
        "\n",
        "# # create a 2D array of multiple window steps\n",
        "# window_indices = window_step + np.expand_dims(np.arange(len(prices)-(WINDOW_SIZE+HORIZON-1)),axis=0).T\n",
        "\n",
        "# # index on the time series with 2D array\n",
        "# windowed_array = prices[window_indices]\n",
        "\n",
        "# # finally, label windowed data\n",
        "# full_windows, full_labels = windowed_array[:,:-HORIZON], x[:,-HORIZON:]\n",
        "\n",
        "# # need train data to be early data and test data to be later data\n",
        "# split_size = int(0.8*len(prices)) # 80% train, 20% test\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "171Bpv56Cbpa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# use tensorflow for forecasting\n",
        "\n",
        "forecast_model = tf.keras.models.Sequential([\n",
        "    layers.Dense(128,kernel_initializer=\"he_normal\",activation=\"relu\"),\n",
        "    layers.Dense(128,kernel_initializer=\"he_normal\",activation=\"relu\"),\n",
        "    layers.Dense(HORIZON)\n",
        "],name=\"forecast_model\")\n",
        "\n",
        "forecast_model.compile(loss=\"mae\",\n",
        "                       optimizer=tf.keras.optimizers.Adam(),\n",
        "                       metrics=[\"mae\"])\n",
        "\n",
        "model = forecast_model\n",
        "data_input = prices\n",
        "forecast_depth = 100\n",
        "\n",
        "# take in the model\n",
        "projection_record = data_input\n",
        "\n",
        "for i in range(forecast_depth):\n",
        "  # recalculate all windows and labels\n",
        "  # create a window of specific window_size\n",
        "  window_step = np.expand_dims(np.arange(WINDOW_SIZE+HORIZON),axis=0)\n",
        "\n",
        "  # create a 2D array of multiple window steps\n",
        "  window_indices = window_step + np.expand_dims(np.arange(len(prices)-(WINDOW_SIZE+HORIZON-1)),axis=0).T\n",
        "\n",
        "  # index on the time series with 2D array\n",
        "  windowed_array = prices[window_indices]\n",
        "\n",
        "  # finally, label windowed data\n",
        "  all_windows, all_labels = windowed_array[:,:-HORIZON], x[:,-HORIZON:]\n",
        "\n",
        "  # make combined dataset\n",
        "  all_dataset_windows = tf.data.Dataset.from_tensor_slices(all_windows)\n",
        "  all_dataset_labels = tf.data.Dataset.from_tensor_slices(all_labels)\n",
        "  all_dataset = tf.data.Dataset.zip((all_dataset_windows,all_dataset_labels)) \n",
        "  all_dataset = all_dataset.batch(128).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "  # train the model on all of the data\n",
        "  model.fit(all_dataset,\n",
        "            epochs=epochs,\n",
        "            verbose=0)\n",
        "\n",
        "  # predict next value based on last WINDOW_SIZE price values\n",
        "  predictor_window = projection_record[-WINDOW_SIZE:]\n",
        "  next_prediction = model.predict(tf.expand_dims(predictor_window,axis=0))\n",
        "\n",
        "  # add predicted price value to end of price list\n",
        "  projection_record = np.append(projection_record,next_prediction)\n",
        "\n",
        "projection_record\n"
      ],
      "metadata": {
        "id": "L3rJYq5uCeIO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# identify and compensate for sequence bias\n",
        "\n",
        "\"\"\"\n",
        "sequence bias is prejudice or favor toward something due to its order within a list\n",
        "(essentially, where appropriate, you want to randomize the order of your train set...\n",
        "this is not possible in time series, i would think...)\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "FAauIIW1ChAe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# adjust the learning rate dynamically in time series, sequence, \n",
        "# and prediction models\n",
        "\n",
        "\"\"\"\n",
        "OPTIONS:\n",
        "include ReduceLROnPlateau callback during fit\n",
        "include LearningRateScheduler callback during fit\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "waCMRVjICjm7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### getting time series data with python csv"
      ],
      "metadata": {
        "id": "x5GcWXZjApsM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# time series with python csv:\n",
        "import csv\n",
        "from datetime import datetime\n",
        "\n",
        "timesteps = []\n",
        "btc_price = []\n",
        "\n",
        "with open(\"/content/BTC_USD_2013-10-01_2021-05-18-CoinDesk.csv\", \"r\") as f:\n",
        "  csv_reader = csv.reader(f, delimiter=\",\") # read in the target CSV\n",
        "  next(csv_reader) # skip first line (this gets rid of the column titles)\n",
        "\n",
        "  for line in csv_reader:\n",
        "    timesteps.append(datetime.strptime(line[1], \"%Y-%m-%d\")) # get the dates as dates (not strings), strptime = string parse time\n",
        "    btc_price.append(float(line[2])) # get the closing price as float\n"
      ],
      "metadata": {
        "id": "lPVELU2LAs_W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "timesteps = timesteps\n",
        "prices = btc_price"
      ],
      "metadata": {
        "id": "sepwuQjrA3-8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### time series train/test sets with python csv"
      ],
      "metadata": {
        "id": "wcaxuTfZBXsd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# need train data to be early data and test data to be later data\n",
        "split_size = int(0.8*len(prices)) # 80% train, 20% test\n",
        "X_train, y_train = timesteps[:split_size], prices[:split_size]\n",
        "X_test, y_test = timesteps[split_size:], prices[split_size:]\n"
      ],
      "metadata": {
        "id": "zXYdcLs7BfLr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Callback construction"
      ],
      "metadata": {
        "id": "paLbRkwAIMpV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ModelCheckpoint\n",
        "\n",
        "save the best weights for the model you're fitting"
      ],
      "metadata": {
        "id": "wnaYG-pfPKLZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "save_path = \"model_experiments\"\n",
        "# need model_name\n",
        "tf.keras.callbacks.ModelCheckpoint(filepath=os.path.join(save_path,model_name),\n",
        "                                            verbose=0,\n",
        "                                            monitor=\"val_loss\",\n",
        "                                            save_best_only=True)"
      ],
      "metadata": {
        "id": "g6btcHi3CpqJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ReduceLROnPlateau\n",
        "\n",
        "when val_loss plateaus, try to improve performance by reducing the learning rate"
      ],
      "metadata": {
        "id": "hfvuxBzgPMko"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\",\n",
        "                                     patience=100,\n",
        "                                     verbose=1)"
      ],
      "metadata": {
        "id": "FhA0jP09ChqD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LearningRateScheduler\n",
        "\n",
        "allows us to find the ideal learning rate"
      ],
      "metadata": {
        "id": "j_7rGffgF-vN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# traverse a set of learning rate values starting from 1e-4, increasing by 10**(epoch/20) every epoch\n",
        "tf.keras.callbacks.LearningRateScheduler(lambda epoch: 1e-4 * 10**(epoch/20))\n"
      ],
      "metadata": {
        "id": "nPTBSd9MGA0o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### EarlyStopping\n",
        "\n",
        "stops epochs when val_loss is no longer decreasing"
      ],
      "metadata": {
        "id": "i4XT2JXmQffa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\",\n",
        "                                 patience=200,\n",
        "                                 restore_best_weights=True)"
      ],
      "metadata": {
        "id": "Xdhe5PWJCb6U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TensorBoard"
      ],
      "metadata": {
        "id": "HdDv9FBnPPPU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import datetime\n",
        "\n",
        "dir_name = \"model_logs\"\n",
        "# need experiment_name\n",
        "log_dir = dir_name + \"/\" + experiment_name + \"/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir)\n",
        "\n",
        "# To upload TensorBoard dev records:\n",
        "# !tensorboard dev upload --logdir ./model_logs \\\n",
        "#   --name \"First deep model on text data\" \\\n",
        "#   --description \"Trying a dense model with an embedding layer\" \\\n",
        "#   --one_shot # exits the uploader when upload has finished\n",
        "\n",
        "# If you need to remove previous experiments:\n",
        "# !tensorboard dev delete --experiment_id EXPERIMENT_ID_TO_DELETE\n"
      ],
      "metadata": {
        "id": "6A-5zLKIDceM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## If you get stuck, try these to improve performance\n",
        "\n",
        "* Change learning rate, or use a learning rate-related callback\n",
        "* Change number of layers in model\n",
        "* Change number of neurons per layer\n",
        "* Change the activation functions\n",
        "* Change the optimization function\n",
        "* Fit on more data\n",
        "* Fit for longer\n",
        "* Normalize the data\n",
        "* Try data augmentation\n",
        "* Change batching?\n",
        "* With time series data, try a trailing moving average?\n",
        "* Try a Dropout layer?"
      ],
      "metadata": {
        "id": "zpVahOqbP3O1"
      }
    }
  ]
}